name: 'CodeCritique Review'
description: 'Automated code review using AI analysis with context-aware embeddings'

branding:
  icon: 'brain'
  color: 'blue'

inputs:
  # AI Configuration
  anthropic-api-key:
    description: 'Anthropic API key for Claude models'
    required: true

  skip-label:
    description: 'Label name to skip AI review (e.g., ai-review-disabled)'
    required: false
    default: 'ai-review-disabled'

  verbose:
    description: 'Show verbose output'
    required: false
    default: 'false'

  model:
    description: 'LLM model to use (e.g., claude-sonnet-4-5)'
    required: false

  max-tokens:
    description: 'LLM max tokens'
    required: false

  cache-ttl:
    description: 'Cache TTL for LLM prompts: "5m" (default, no extra cost) or "1h" (extended, extra cost for cache writes)'
    required: false
    default: '5m'

  concurrency:
    description: 'Concurrency for processing multiple files'
    required: false

  custom-docs:
    description: 'Custom documents to provide LLM instructions (format: "title:path,title:path")'
    required: false
    default: ''

outputs:
  # Review Results
  comments-posted:
    description: 'Number of review comments posted'
    value: ${{ steps.review.outputs.comments-posted }}

  issues-found:
    description: 'Total number of issues found'
    value: ${{ steps.review.outputs.issues-found }}

  files-analyzed:
    description: 'Number of files analyzed'
    value: ${{ steps.review.outputs.files-analyzed }}

  # Performance Metrics

  embedding-cache-hit:
    description: 'Whether embeddings were found and used'
    value: ${{ steps.review.outputs.embedding-cache-hit }}

  # Quality Metrics
  review-score:
    description: 'Overall review score (0-100)'
    value: ${{ steps.review.outputs.review-score }}

  security-issues:
    description: 'Number of security issues found'
    value: ${{ steps.review.outputs.security-issues }}

  performance-issues:
    description: 'Number of performance issues found'
    value: ${{ steps.review.outputs.performance-issues }}

  maintainability-issues:
    description: 'Number of maintainability issues found'
    value: ${{ steps.review.outputs.maintainability-issues }}

  # Artifacts
  feedback-artifact-uploaded:
    description: 'Whether feedback artifact was uploaded'
    value: ${{ steps.review.outputs.feedback-artifact-uploaded }}

  review-report-path:
    description: 'Path to the detailed review report'
    value: ${{ steps.review.outputs.review-report-path }}

runs:
  using: 'composite'
  steps:
    # Set up environment variables for reuse across steps
    - name: Set environment variables
      shell: bash
      run: |
        # Feedback cache key prefix (used for restore/save) - scoped to this PR
        echo "FEEDBACK_CACHE_KEY=ai-review-feedback-${{ github.repository_id }}-pr-${{ github.event.pull_request.number }}" >> $GITHUB_ENV

    # Check if PR has skip label and exit early if present
    - name: Check for skip label
      if: github.event.pull_request.labels
      shell: bash
      env:
        PR_LABELS: ${{ toJSON(github.event.pull_request.labels.*.name) }}
        SKIP_LABEL: ${{ inputs.skip-label }}
      run: |
        if echo "$PR_LABELS" | jq -e --arg label "$SKIP_LABEL" 'index($label)' > /dev/null; then
          echo "‚è≠Ô∏è  Skipping AI review - PR has '$SKIP_LABEL' label"
          exit 0
        fi
        echo "‚úÖ No skip label found, proceeding with review"

    # Create a symlink to the action repository to access sibling composite actions.
    # This workaround is needed because GitHub Actions doesn't support relative paths
    # to sibling actions when the action is used from external repositories.
    # The symlink allows us to reference ./.action-repo/.github/actions/setup-tool
    - name: Link current action repo
      shell: bash
      run: ln -fs "$(realpath "${{ github.action_path }}/../../..")" "$GITHUB_WORKSPACE/.action-repo"

    - name: Setup CodeCritique Tool
      id: setup-tool
      uses: ./.action-repo/.github/actions/setup-tool
      with:
        github-token: ${{ github.token }}

    - name: Cleanup link
      if: always()
      shell: bash
      run: rm -f "$GITHUB_WORKSPACE/.action-repo"

    - name: Restore feedback cache
      id: feedback-cache
      uses: actions/cache/restore@v4
      with:
        path: ${{ github.workspace }}/.ai-feedback
        # Key includes run_id and run_attempt to ensure uniqueness even on re-runs
        key: ${{ env.FEEDBACK_CACHE_KEY }}-${{ github.run_id }}-${{ github.run_attempt }}
        # restore-keys matches the most recent cache with this prefix
        restore-keys: |
          ${{ env.FEEDBACK_CACHE_KEY }}-
      continue-on-error: true

    - name: Run AI Code Review
      id: review
      shell: bash
      working-directory: ${{ steps.setup-tool.outputs.tool-root }}
      env:
        GITHUB_TOKEN: ${{ github.token }}
        ANTHROPIC_API_KEY: ${{ inputs.anthropic-api-key }}
        GITHUB_WORKSPACE_PATH: ${{ github.workspace }}
        CI: true
        DEBUG: ${{ inputs.verbose }}
        VERBOSE: ${{ inputs.verbose }}
      run: |
        # Build CLI command arguments - analyze workspace directly
        CLI_ARGS="analyze -b ${{ github.head_ref }} --directory \"$GITHUB_WORKSPACE\""

        # Add output format and output file (always JSON)
        CLI_ARGS="$CLI_ARGS --output json --output-file review_output.json"

        # Add verbose flag if enabled
        if [ "${{ inputs.verbose }}" = "true" ]; then
          CLI_ARGS="$CLI_ARGS --verbose"
        fi

        # Add model configuration
        if [ -n "${{ inputs.model }}" ]; then
          CLI_ARGS="$CLI_ARGS --model ${{ inputs.model }}"
        fi

        # Only add optional parameters if they have values
        if [ -n "${{ inputs.max-tokens }}" ]; then
          CLI_ARGS="$CLI_ARGS --max-tokens ${{ inputs.max-tokens }}"
        fi

        if [ -n "${{ inputs.cache-ttl }}" ]; then
          CLI_ARGS="$CLI_ARGS --cache-ttl ${{ inputs.cache-ttl }}"
        fi

        if [ -n "${{ inputs.concurrency }}" ]; then
          CLI_ARGS="$CLI_ARGS --concurrency ${{ inputs.concurrency }}"
        fi

        # Add custom documents if specified
        if [ -n "${{ inputs.custom-docs }}" ]; then
          IFS=',' read -ra DOCS <<< "${{ inputs.custom-docs }}"
          for doc in "${DOCS[@]}"; do
            if [ -n "$doc" ]; then
              CLI_ARGS="$CLI_ARGS --doc \"$doc\""
            fi
          done
        fi

        # Add feedback parameters
        CLI_ARGS="$CLI_ARGS --track-feedback"
        CLI_ARGS="$CLI_ARGS --feedback-path \"$GITHUB_WORKSPACE/.ai-feedback\""

        # Run the AI code review
        echo "ü§ñ Running AI Code Review..."
        echo "Command: node src/index.js $CLI_ARGS"

        # Execute the command and capture output
        # Run the CLI - JSON output goes to file, logs to console
        eval "node src/index.js $CLI_ARGS" || {
          echo "‚ùå AI Code Review failed"
          exit 1
        }

        echo "‚úÖ AI Code Review completed"

        # Count results and set outputs from JSON
        FILES_ANALYZED=$(jq '.summary.totalFilesReviewed // 0' review_output.json 2>/dev/null || echo "0")
        ISSUES_FOUND=$(jq '.summary.totalIssues // 0' review_output.json 2>/dev/null || echo "0")

        # Set action outputs
        echo "files-analyzed=$FILES_ANALYZED" >> $GITHUB_OUTPUT
        echo "issues-found=$ISSUES_FOUND" >> $GITHUB_OUTPUT
        # Check if embeddings were successfully downloaded
        EMBEDDING_AVAILABLE="${{ steps.setup-tool.outputs.embeddings-available }}"

        echo "embedding-cache-hit=$EMBEDDING_AVAILABLE" >> $GITHUB_OUTPUT

        if [ "$EMBEDDING_AVAILABLE" != "true" ]; then
          echo "‚ö†Ô∏è  WARNING: No embeddings available! Analysis will be less effective."
          echo "   Consider running the embedding generation workflow first:"
          echo "   https://github.com/${{ github.repository }}/actions/workflows/generate-embeddings.yml"
        fi
        echo "comments-posted=0" >> $GITHUB_OUTPUT
        echo "review-score=0" >> $GITHUB_OUTPUT
        echo "security-issues=0" >> $GITHUB_OUTPUT
        echo "performance-issues=0" >> $GITHUB_OUTPUT
        echo "maintainability-issues=0" >> $GITHUB_OUTPUT
        echo "feedback-artifact-uploaded=false" >> $GITHUB_OUTPUT
        echo "review-report-path=review_output.json" >> $GITHUB_OUTPUT

        # Display results
        if [ "${{ inputs.verbose }}" = "true" ]; then
          echo "üìä Review Results:"
          echo "  Files analyzed: $FILES_ANALYZED"
          echo "  Issues found: $ISSUES_FOUND"
        fi

        # Show the actual output
        echo "üìã Review Output:"
        cat review_output.json

    - name: Post PR Comments
      if: ${{ github.event_name == 'pull_request' }}
      uses: actions/github-script@v7
      env:
        REVIEW_OUTPUT_PATH: ${{ steps.setup-tool.outputs.tool-root }}/review_output.json
      with:
        github-token: ${{ github.token }}
        script: |
          const script = await import('${{ github.action_path }}/post-comments.js');
          await script.default({ github, context, core });

    - name: Prepare feedback for cache
      id: prepare-feedback
      shell: bash
      run: |
        # Consolidate feedback files into the cache directory
        FEEDBACK_DIR="${{ github.workspace }}/.ai-feedback"
        mkdir -p "$FEEDBACK_DIR"

        # Copy any new feedback files from tool root
        TOOL_FEEDBACK="${{ steps.setup-tool.outputs.tool-root }}/feedback-*.json"
        for f in $TOOL_FEEDBACK; do
          if [ -f "$f" ]; then
            cp "$f" "$FEEDBACK_DIR/"
          fi
        done

        # Check if we have feedback to save
        if ls "$FEEDBACK_DIR"/feedback-*.json 1> /dev/null 2>&1; then
          echo "has-feedback=true" >> $GITHUB_OUTPUT
          echo "‚úÖ Feedback files ready for caching"
        else
          echo "has-feedback=false" >> $GITHUB_OUTPUT
          echo "‚ö†Ô∏è  No feedback files to cache"
        fi
      continue-on-error: true

    - name: Save feedback cache
      if: steps.prepare-feedback.outputs.has-feedback == 'true'
      uses: actions/cache/save@v4
      with:
        path: ${{ github.workspace }}/.ai-feedback
        key: ${{ env.FEEDBACK_CACHE_KEY }}-${{ github.run_id }}-${{ github.run_attempt }}
      continue-on-error: true

    - name: Upload review report
      uses: actions/upload-artifact@v4
      with:
        name: ai-review-report-${{ github.event.pull_request.number || 'branch' }}
        path: ${{ steps.setup-tool.outputs.tool-root }}/review_output.json
        retention-days: 7
      continue-on-error: true
